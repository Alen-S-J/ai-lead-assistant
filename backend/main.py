from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional
from fastapi.middleware.cors import CORSMiddleware
import sys
import os

# Add parent dir to path to import from adjacent folders
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

# Import from the specific assistant modules
# Note: We might need to adjust __init__.py or paths, but for now let's mock the logic or try dynamic imports
# For a robust app, we should rely on the PROMPT.md and logic defined in main.py of those folders.
# Since those main.py were demos, I will reimplement the core logic here for the API, 
# reading the PROMPT.md files as the 'source of truth' for the system prompts.

app = FastAPI(title="Lead Follow-Up Assistant API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class GenerateRequest(BaseModel):
    lead_name: str
    product_name: str
    interaction_history: str
    tone: str
    goal: str
    assistant_type: str # 'rag', 'finetuned', 'hybrid'

def read_prompt_template(folder_name):
    path = os.path.join(os.path.dirname(__file__), f"../../{folder_name}/PROMPT.md")
    with open(path, "r", encoding='utf-8') as f:
        return f.read()

def mock_rag_retrieval(query):
    return f"Retrieved context for {query}. Pricing: Starter $29/mo, Pro $99/mo. Case Study: TechFlow saved 40% time."

def mock_llm_generation(prompt):
    # In a real app, this calls OpenAI/Gemini/etc.
    # Here we return a deterministic simulation for the demo.
    return f"This is a simulated response generated by the LLM based on the prompt.\n\nKey details included from RAG/Context."

@app.post("/generate")
def generate_response(req: GenerateRequest):
    if req.assistant_type == 'rag':
        prompt_md = read_prompt_template('rag_assistant')
        rag_context = mock_rag_retrieval(req.product_name)
        # Simple string replacement for demo (robust templating would use Jinja2)
        # We really just want to show that we are 'using' the prompt file.
        final_prompt = f"SYSTEM PROMPT:\n{prompt_md}\n\nUSER INPUT:\nLead: {req.lead_name}\nContext: {rag_context}\n..."
        
        response_text = f"Hi {req.lead_name},\n\nHope you're doing great! Based on our documentation, {req.product_name} starts at $29/mo—would that fit your budget?\n\nCheers,\nAcme Team"
        
    elif req.assistant_type == 'finetuned':
        prompt_md = read_prompt_template('finetuned_assistant')
        final_prompt = f"SYSTEM PROMPT:\n{prompt_md}\n\nUser Input: {req.lead_name}"
        response_text = f"Hey {req.lead_name},\n\nJust checking in! I recall you were interested in {req.product_name}. How is your search going?\n\nBest,\nAcme Team"

    elif req.assistant_type == 'hybrid':
        prompt_md = read_prompt_template('hybrid_assistant')
        rag_context = mock_rag_retrieval(req.product_name)
        final_prompt = f"SYSTEM PROMPT:\n{prompt_md}\n\nRAG: {rag_context}\nUser: {req.lead_name}"
        response_text = f"Hello {req.lead_name},\n\nWe recently helped a client like you save 40% on billing time. Our Pro plan is $99/mo—interested in a demo?\n\nRegards,\nAcme Team"
    
    else:
        raise HTTPException(status_code=400, detail="Invalid assistant type")

    return {
        "generated_response": response_text,
        "debug_prompt": final_prompt[:500] + "...", # Show snippet
        "retrieved_context": mock_rag_retrieval(req.product_name) if req.assistant_type != 'finetuned' else None
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
